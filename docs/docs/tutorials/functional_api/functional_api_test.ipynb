{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain_core langchain-anthropic langgraph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"ANTHROPIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "llm = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla Agent\n",
    "\n",
    "* No orchestration framework \n",
    "* Use LangChain to bind tools and specify tools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "# Define tools\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def divide(a: int, b: int) -> float:\n",
    "    \"\"\"Divide a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a / b\n",
    "\n",
    "# Augment the LLM with tools\n",
    "tools = [add, multiply, divide]\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Add 3 and 4.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "[{'text': \"I'll help you add those numbers using the `add` function.\", 'type': 'text'}, {'id': 'toolu_01Lr23DbYsJvGSuzwQdXqDvH', 'input': {'a': 3, 'b': 4}, 'name': 'add', 'type': 'tool_use'}]\n",
      "Tool Calls:\n",
      "  add (toolu_01Lr23DbYsJvGSuzwQdXqDvH)\n",
      " Call ID: toolu_01Lr23DbYsJvGSuzwQdXqDvH\n",
      "  Args:\n",
      "    a: 3\n",
      "    b: 4\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: add\n",
      "\n",
      "7\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The sum of 3 and 4 is 7.\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import add_messages\n",
    "from langchain_core.messages import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    BaseMessage,\n",
    "    ToolCall,\n",
    ")\n",
    "\n",
    "def call_llm(messages: list[BaseMessage]):\n",
    "    \"\"\"LLM decides whether to call a tool or not\"\"\"\n",
    "    return llm_with_tools.invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n",
    "            )\n",
    "        ]\n",
    "        + messages\n",
    "    )\n",
    "\n",
    "def call_tool(tool_call: ToolCall):\n",
    "    \"\"\"Performs the tool call\"\"\"\n",
    "\n",
    "    tool = tools_by_name[tool_call[\"name\"]]\n",
    "    return tool.invoke(tool_call)\n",
    "\n",
    "def agent(messages: list[BaseMessage]):\n",
    "    \"\"\" Tool calling agent \"\"\"\n",
    "    llm_response = call_llm(messages)\n",
    "\n",
    "    while True:\n",
    "        if not llm_response.tool_calls:\n",
    "            break\n",
    "\n",
    "        # Execute tools\n",
    "        tool_results = [\n",
    "            call_tool(tool_call) for tool_call in llm_response.tool_calls\n",
    "        ]\n",
    "        messages = add_messages(messages, [llm_response, *tool_results])\n",
    "        llm_response = call_llm(messages)\n",
    "\n",
    "    messages = add_messages(messages, llm_response)\n",
    "    return messages\n",
    "\n",
    "# Stream\n",
    "messages = agent([HumanMessage(content=\"Add 3 and 4.\")])\n",
    "for m in messages:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent with short-term memory (within a thread)\n",
    "\n",
    "* LangGraph persistence layer \n",
    "\n",
    "`@entrypoint` \n",
    "* Decorator indicates the start of a workflow / agent \n",
    "* Produces a Pregel object, an abstraction for managing a few things\n",
    "* Execution -- Syncronous (invoke), Async (ainvoke), streaming (stream)\n",
    "* State -- Checkpointing, Human in the loop (interrupt)\n",
    "\n",
    "[Optional: `@entrypoint.final`](https://langchain-ai.github.io/langgraph/concepts/functional_api/#entrypointfinal)\n",
    "* Can be used to specify what to return vs what to checkpoint \n",
    "\n",
    "`@task`\n",
    "* Results from tasks are saved as checkpoints\n",
    "* Important for caching results (time-consuming operations)\n",
    "* Support streaming updates from tasks\n",
    "* Support tracing\n",
    "\n",
    "Calling a task -- \n",
    "* When you call a task, it returns immediately with a future object.\n",
    "* A future is a placeholder for a result that will be available later.\n",
    "* `.result()` marks where in the code you actually need the task's result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langgraph.func import entrypoint, task # New \n",
    "from langgraph.checkpoint.memory import MemorySaver # New \n",
    "\n",
    "@task # New\n",
    "def call_llm(messages: list[BaseMessage]):\n",
    "    \"\"\"LLM decides whether to call a tool or not\"\"\"\n",
    "    return llm_with_tools.invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n",
    "            )\n",
    "        ]\n",
    "        + messages\n",
    "    )\n",
    "\n",
    "@task # New\n",
    "def call_tool(tool_call: ToolCall):\n",
    "    \"\"\"Performs the tool call\"\"\"\n",
    "\n",
    "    tool = tools_by_name[tool_call[\"name\"]]\n",
    "    return tool.invoke(tool_call)\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "@entrypoint(checkpointer=checkpointer) # New \n",
    "def agent(messages: list[BaseMessage], previous: list[BaseMessage]): # New \n",
    "    \"\"\" Tool calling agent \"\"\"\n",
    "\n",
    "    # Add previous messages from short-term memory to the current messages\n",
    "    if previous is not None:\n",
    "        messages = add_messages(previous, messages)\n",
    "    \n",
    "    # Call the LLM\n",
    "    llm_response = call_llm(messages).result()\n",
    "\n",
    "    while True:\n",
    "        if not llm_response.tool_calls:\n",
    "            break\n",
    "\n",
    "        # Execute tools\n",
    "        tool_results = [\n",
    "            call_tool(tool_call).result() for tool_call in llm_response.tool_calls\n",
    "        ]\n",
    "        messages = add_messages(messages, [llm_response, *tool_results])\n",
    "        llm_response = call_llm(messages).result()\n",
    "\n",
    "    messages = add_messages(messages, llm_response)\n",
    "\n",
    "    # Return LLM response and save the full message history\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Add 3 and 4.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "[{'text': \"I'll help you add 3 and 4 using the `add` function.\", 'type': 'text'}, {'id': 'toolu_013z1syyD8SvfDKdysUSwyeq', 'input': {'a': 3, 'b': 4}, 'name': 'add', 'type': 'tool_use'}]\n",
      "Tool Calls:\n",
      "  add (toolu_013z1syyD8SvfDKdysUSwyeq)\n",
      " Call ID: toolu_013z1syyD8SvfDKdysUSwyeq\n",
      "  Args:\n",
      "    a: 3\n",
      "    b: 4\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: add\n",
      "\n",
      "7\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The sum of 3 and 4 is 7.\n"
     ]
    }
   ],
   "source": [
    "# Thread ID\n",
    "thread_id = str(uuid.uuid4())\n",
    "\n",
    "# Config\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "# Run with checkpointer to persist state in memory\n",
    "messages = agent.invoke([HumanMessage(content=\"Add 3 and 4.\")], config)\n",
    "for m in messages:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Add 3 and 4.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "[{'text': \"I'll help you add 3 and 4 using the `add` function.\", 'type': 'text'}, {'id': 'toolu_013z1syyD8SvfDKdysUSwyeq', 'input': {'a': 3, 'b': 4}, 'name': 'add', 'type': 'tool_use'}]\n",
      "Tool Calls:\n",
      "  add (toolu_013z1syyD8SvfDKdysUSwyeq)\n",
      " Call ID: toolu_013z1syyD8SvfDKdysUSwyeq\n",
      "  Args:\n",
      "    a: 3\n",
      "    b: 4\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: add\n",
      "\n",
      "7\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The sum of 3 and 4 is 7.\n"
     ]
    }
   ],
   "source": [
    "# Get the last checkpoint, which contains the full message history\n",
    "agent_state = agent.get_state(config)\n",
    "for m in agent_state.values:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Add 3 and 4.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "[{'text': \"I'll help you add 3 and 4 using the `add` function.\", 'type': 'text'}, {'id': 'toolu_013z1syyD8SvfDKdysUSwyeq', 'input': {'a': 3, 'b': 4}, 'name': 'add', 'type': 'tool_use'}]\n",
      "Tool Calls:\n",
      "  add (toolu_013z1syyD8SvfDKdysUSwyeq)\n",
      " Call ID: toolu_013z1syyD8SvfDKdysUSwyeq\n",
      "  Args:\n",
      "    a: 3\n",
      "    b: 4\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: add\n",
      "\n",
      "7\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The sum of 3 and 4 is 7.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Take the result and multiply it by 2.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "[{'text': \"I'll multiply the previous result (7) by 2 using the `multiply` function.\", 'type': 'text'}, {'id': 'toolu_01RhScbTXybkGxYG6RA1tQpM', 'input': {'a': 7, 'b': 2}, 'name': 'multiply', 'type': 'tool_use'}]\n",
      "Tool Calls:\n",
      "  multiply (toolu_01RhScbTXybkGxYG6RA1tQpM)\n",
      " Call ID: toolu_01RhScbTXybkGxYG6RA1tQpM\n",
      "  Args:\n",
      "    a: 7\n",
      "    b: 2\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "14\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The result of multiplying 7 by 2 is 14.\n"
     ]
    }
   ],
   "source": [
    "# Continue with the same thread\n",
    "messages = agent.invoke([HumanMessage(content=\"Take the result and multiply it by 2.\")], config)\n",
    "for m in messages:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent with HITL\n",
    "\n",
    "* Add interrupt to the workflow to allow for HITL.\n",
    "* Re-executes from the start of the entrypoint.\n",
    "* Output of each `@task` is cached / saved as a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.types import interrupt\n",
    "\n",
    "@task\n",
    "def call_llm(messages: list[BaseMessage]):\n",
    "    \"\"\"LLM decides whether to call a tool or not\"\"\"\n",
    "    print(\"Calling LLM or using cached LLM output!\")\n",
    "    return llm_with_tools.invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n",
    "            )\n",
    "        ]\n",
    "        + messages\n",
    "    )\n",
    "\n",
    "@task\n",
    "def call_tool(tool_call: ToolCall):\n",
    "    \"\"\"Performs the tool call\"\"\"\n",
    "    print(\"Calling tool or using cached tool output!\")\n",
    "    # Interrupt the workflow to get a review from a human.\n",
    "    is_approved = interrupt({ # New \n",
    "            # Any json-serializable payload provided to interrupt as argument.\n",
    "            # It will be surfaced on the client side as an Interrupt when streaming data\n",
    "            # from the workflow.\n",
    "            \"tool_call\": tool_call, # The tool call we want reviewed.\n",
    "            # We can add any additional information that we need.\n",
    "            # For example, introduce a key called \"action\" with some instructions.\n",
    "            \"action\": \"Please approve/reject the tool call\",\n",
    "        })\n",
    "    \n",
    "    if is_approved:\n",
    "        tool = tools_by_name[tool_call[\"name\"]]\n",
    "        return tool.invoke(tool_call)\n",
    "    else:\n",
    "        return \"Tool call rejected\"\n",
    "\n",
    "@entrypoint(checkpointer=MemorySaver())  \n",
    "def agent(messages: list[BaseMessage], previous: list[BaseMessage]): \n",
    "    \"\"\" Tool calling agent \"\"\"\n",
    "\n",
    "    # Add previous messages from short-term memory to the current messages\n",
    "    if previous is not None:\n",
    "        messages = add_messages(previous, messages)\n",
    "    \n",
    "    # Call the LLM\n",
    "    llm_response = call_llm(messages).result()\n",
    "\n",
    "    while True:\n",
    "        if not llm_response.tool_calls:\n",
    "            break\n",
    "\n",
    "        # Execute tools\n",
    "        tool_results = [\n",
    "            call_tool(tool_call).result() for tool_call in llm_response.tool_calls\n",
    "        ]\n",
    "        messages = add_messages(messages, [llm_response, *tool_results])\n",
    "        llm_response = call_llm(messages).result()\n",
    "\n",
    "    messages = add_messages(messages, llm_response)\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling LLM or using cached LLM output!\n",
      "Calling tool or using cached tool output!\n",
      "{'tool_call': {'name': 'add', 'args': {'a': 3, 'b': 4}, 'id': 'toolu_017uJdvp4SfEyPLVWamxBjGT', 'type': 'tool_call'}, 'action': 'Please approve/reject the tool call'}\n"
     ]
    }
   ],
   "source": [
    "# Thread ID\n",
    "thread_id = str(uuid.uuid4())\n",
    "\n",
    "# Config\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "# Run until the interrupt \n",
    "for item in agent.stream([HumanMessage(content=\"Add 3 and 4.\")], config, stream_mode=\"updates\"):\n",
    "    if '__interrupt__' in item:\n",
    "        print(item['__interrupt__'][0].value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling tool or using cached tool output!\n",
      "Calling LLM or using cached LLM output!\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The sum of 3 and 4 is 7.\n"
     ]
    }
   ],
   "source": [
    "from langgraph.types import Command\n",
    "for item in agent.stream(Command(resume=True), config, stream_mode=\"updates\"):\n",
    "    if 'agent' in item:\n",
    "        item['agent'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time travel\n",
    "\n",
    "* It can be useful to add time travel to the workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def call_llm(messages: list[BaseMessage]):\n",
    "    \"\"\"LLM decides whether to call a tool or not\"\"\"\n",
    "    return llm_with_tools.invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n",
    "            )\n",
    "        ]\n",
    "        + messages\n",
    "    )\n",
    "\n",
    "@task\n",
    "def call_tool(tool_call: ToolCall):\n",
    "    \"\"\"Performs the tool call\"\"\"\n",
    "    tool = tools_by_name[tool_call[\"name\"]]\n",
    "    return tool.invoke(tool_call)\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "@entrypoint(checkpointer=checkpointer) \n",
    "def agent(messages: list[BaseMessage], previous: list[BaseMessage]): # New \n",
    "    \"\"\" Tool calling agent \"\"\"\n",
    "    # Add previous messages from short-term memory to the current messages\n",
    "    if previous is not None:\n",
    "        messages = add_messages(previous, messages)\n",
    "    \n",
    "    # Call the LLM\n",
    "    llm_response = call_llm(messages).result()\n",
    "\n",
    "    while True:\n",
    "        if not llm_response.tool_calls:\n",
    "            break\n",
    "\n",
    "        # Execute tools\n",
    "        tool_results = [\n",
    "            call_tool(tool_call).result() for tool_call in llm_response.tool_calls\n",
    "        ]\n",
    "        messages = add_messages(messages, [llm_response, *tool_results])\n",
    "        llm_response = call_llm(messages).result()\n",
    "\n",
    "    messages = add_messages(messages, llm_response)\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Add 3 and 4.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "[{'text': \"I'll help you add 3 and 4 using the `add` function.\", 'type': 'text'}, {'id': 'toolu_015EBCekAyhHGVExDSCbSppi', 'input': {'a': 3, 'b': 4}, 'name': 'add', 'type': 'tool_use'}]\n",
      "Tool Calls:\n",
      "  add (toolu_015EBCekAyhHGVExDSCbSppi)\n",
      " Call ID: toolu_015EBCekAyhHGVExDSCbSppi\n",
      "  Args:\n",
      "    a: 3\n",
      "    b: 4\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: add\n",
      "\n",
      "7\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The sum of 3 and 4 is 7.\n"
     ]
    }
   ],
   "source": [
    "# Thread ID\n",
    "thread_id = str(uuid.uuid4())\n",
    "\n",
    "# Config\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "# Run with checkpointer to persist state in memory\n",
    "messages = agent.invoke([HumanMessage(content=\"Add 3 and 4.\")], config)\n",
    "for m in messages:\n",
    "    m.pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The result of multiplying 7 by 2 is 14.\n"
     ]
    }
   ],
   "source": [
    "# Second turn\n",
    "for item in agent.stream([HumanMessage(content=\"Take the result and multiply it by 2.\")], config, stream_mode=\"updates\"):\n",
    "    if 'agent' in item:\n",
    "        item['agent'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': 'ac13180b-31f0-4ff5-8d71-0354addc0a59',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1efddb89-50bb-6c16-8001-4b2a0f971b4e'}}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fork and do alternative second turn\n",
    "to_fork_from = list(agent.get_state_history(config))[1].config\n",
    "to_fork_from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Add 3 and 4.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "[{'text': \"I'll help you add 3 and 4 using the `add` function.\", 'type': 'text'}, {'id': 'toolu_015EBCekAyhHGVExDSCbSppi', 'input': {'a': 3, 'b': 4}, 'name': 'add', 'type': 'tool_use'}]\n",
      "Tool Calls:\n",
      "  add (toolu_015EBCekAyhHGVExDSCbSppi)\n",
      " Call ID: toolu_015EBCekAyhHGVExDSCbSppi\n",
      "  Args:\n",
      "    a: 3\n",
      "    b: 4\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: add\n",
      "\n",
      "7\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The sum of 3 and 4 is 7.\n"
     ]
    }
   ],
   "source": [
    "# Get the last checkpoint, which contains the full message history\n",
    "agent_state = agent.get_state(to_fork_from)\n",
    "for m in agent_state.values:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The result of multiplying 7 by 2 is 14.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The result of multiplying 7 by 2 is 14.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The result of multiplying 7 by 2 is 14.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The result of multiplying 7 by 2 is 14.\n"
     ]
    }
   ],
   "source": [
    "# Re-run the workflow from the fork\n",
    "for chunk in agent.stream([HumanMessage(content=\"Take the result and multiply it by 3.\")], to_fork_from, stream_mode=\"updates\"):\n",
    "    if 'agent' in item:\n",
    "        item['agent'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Add 3 and 4.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "[{'text': \"I'll help you add 3 and 4 using the `add` function.\", 'type': 'text'}, {'id': 'toolu_015EBCekAyhHGVExDSCbSppi', 'input': {'a': 3, 'b': 4}, 'name': 'add', 'type': 'tool_use'}]\n",
      "Tool Calls:\n",
      "  add (toolu_015EBCekAyhHGVExDSCbSppi)\n",
      " Call ID: toolu_015EBCekAyhHGVExDSCbSppi\n",
      "  Args:\n",
      "    a: 3\n",
      "    b: 4\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: add\n",
      "\n",
      "7\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The sum of 3 and 4 is 7.\n"
     ]
    }
   ],
   "source": [
    "agent_state = agent.get_state(to_fork_from)\n",
    "for m in agent_state.values:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent with HITL and Long-term memory (across threads)\n",
    "\n",
    "* Add interrupt to the workflow to allow for HITL\n",
    "* Add tool for [long-term memory](https://langchain-ai.github.io/langgraph/concepts/memory/#long-term-memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from typing import Annotated, Optional\n",
    "\n",
    "from langchain_core.tools import InjectedToolArg\n",
    "from langgraph.store.base import BaseStore\n",
    "\n",
    "@tool \n",
    "def upsert_memory(\n",
    "    content: str,\n",
    "    *,\n",
    "    memory_id: Optional[uuid.UUID] = None,\n",
    "    # Hide these arguments from the model.\n",
    "    store: Annotated[BaseStore, InjectedToolArg],\n",
    "):\n",
    "    \"\"\"Upsert a memory in the database.\n",
    "\n",
    "    If a memory conflicts with an existing one, then just UPDATE the\n",
    "    existing one by passing in memory_id - don't create two memories\n",
    "    that are the same. If the user corrects a memory, UPDATE it.\n",
    "\n",
    "    Args:\n",
    "        content: The main content of the memory. For example:\n",
    "            \"User expressed interest in learning about French.\"\n",
    "        memory_id: ONLY PROVIDE IF UPDATING AN EXISTING MEMORY.\n",
    "        The memory to overwrite.\n",
    "    \"\"\"\n",
    "    mem_id = memory_id or uuid.uuid4()\n",
    "\n",
    "    # BaseStore is a LangGraph persistence layer\n",
    "    store.put(\n",
    "        (\"memories\"),\n",
    "        key=str(mem_id),\n",
    "        value={\"content\": content},\n",
    "    )\n",
    "    return f\"Stored memory {mem_id}\"\n",
    "\n",
    "# Augment the LLM with tools\n",
    "tools = [upsert_memory]\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "llm_with_memory_tool = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.store.memory import InMemoryStore # New \n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "@task\n",
    "def call_llm(messages: list[BaseMessage]):\n",
    "    \"\"\"LLM decides whether to call a tool or not\"\"\"\n",
    "    return llm_with_memory_tool.invoke( # New \n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=\"You are a helpful assistant tasked with storing memories.\" # New \n",
    "            )\n",
    "        ]\n",
    "        + messages\n",
    "    )\n",
    "\n",
    "@task\n",
    "def call_tool(tool_call: ToolCall, store: BaseStore):\n",
    "\n",
    "    # Interrupt the workflow to get a review from a human.\n",
    "    is_approved = interrupt({ # New \n",
    "            # Any json-serializable payload provided to interrupt as argument.\n",
    "            # It will be surfaced on the client side as an Interrupt when streaming data\n",
    "            # from the workflow.\n",
    "            \"tool_call\": tool_call, # The tool call we want reviewed.\n",
    "            # We can add any additional information that we need.\n",
    "            # For example, introduce a key called \"action\" with some instructions.\n",
    "            \"action\": \"Please approve/reject the tool call\",\n",
    "        })\n",
    "    \n",
    "    if is_approved:\n",
    "\n",
    "        print(\"Tool call approved, Memory Added!\")\n",
    "\n",
    "        tool = tools_by_name[tool_call[\"name\"]]\n",
    "        tool.invoke({**tool_call[\"args\"], \"store\": store})\n",
    "\n",
    "        # Tool message provides confirmation to the model that the actions it took were completed\n",
    "        results = ToolMessage(content=tool_call[\"args\"][\"content\"], tool_call_id=tool_call[\"id\"])\n",
    "        return results\n",
    "    else: \n",
    "        return \"Tool call rejected\"\n",
    "\n",
    "in_memory_store = InMemoryStore()\n",
    "@entrypoint(checkpointer=MemorySaver(), store=in_memory_store)  \n",
    "def agent(messages: list[BaseMessage], previous: list[BaseMessage], store: BaseStore): \n",
    "    \"\"\" Tool calling agent \"\"\"\n",
    "\n",
    "    # Add previous messages from short-term memory to the current messages\n",
    "    if previous is not None:\n",
    "        messages = add_messages(previous, messages)\n",
    "    \n",
    "    # New \n",
    "    # Retrieve the most recent memories for context\n",
    "    memories = store.search( \n",
    "        (\"memories\"),\n",
    "        limit=10,\n",
    "    )\n",
    "\n",
    "    # New\n",
    "    # Format memories for inclusion in the prompt\n",
    "    formatted = \"\\n\".join(f\"[{mem.key}]: {mem.value} (similarity: {mem.score})\" for mem in memories)\n",
    "    if formatted:\n",
    "        formatted = f\"\"\"\n",
    "<memories>\n",
    "{formatted}\n",
    "</memories>\"\"\"\n",
    "\n",
    "    # New\n",
    "    # Call the LLM\n",
    "    llm_response = call_llm([SystemMessage(content=f\"Here is some context for you about the user: {formatted}\"), *messages]).result()\n",
    "\n",
    "    while True:\n",
    "        if not llm_response.tool_calls:\n",
    "            break\n",
    "\n",
    "        # Execute tools\n",
    "        tool_results = [\n",
    "            call_tool(tool_call=tool_call, store=store).result() for tool_call in llm_response.tool_calls\n",
    "        ]\n",
    "        messages = add_messages(messages, [llm_response, *tool_results])\n",
    "        llm_response = call_llm(messages).result()\n",
    "\n",
    "    messages = add_messages(messages, llm_response)\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tool_call': {'name': 'upsert_memory', 'args': {'content': \"User's name is Lance.\"}, 'id': 'toolu_01Ppw9sRsBnkpmfUPmeHZVyR', 'type': 'tool_call'}, 'action': 'Please approve/reject the tool call'}\n"
     ]
    }
   ],
   "source": [
    "# Thread ID\n",
    "thread_id = str(uuid.uuid4())\n",
    "\n",
    "# Config\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "# Run until the interrupt \n",
    "for item in agent.stream([HumanMessage(content=\"Hi my name is Lance.\")], config, stream_mode=\"updates\"):\n",
    "    if '__interrupt__' in item:\n",
    "        print(item['__interrupt__'][0].value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in agent.stream(Command(resume=True), config, stream_mode=\"updates\"):\n",
    "    if 'agent' in item:\n",
    "        item['agent'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
